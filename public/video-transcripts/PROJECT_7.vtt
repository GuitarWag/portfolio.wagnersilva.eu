WEBVTT

NOTE
This file was generated by Descript <www.descript.com>

00:00:00.049 --> 00:00:02.360
smart API Integration.

00:00:02.479 --> 00:00:06.999
So this one was a little bit challenging.

00:00:08.000 --> 00:00:12.750
And the big pain point on
this was actually the PASCOM

00:00:12.770 --> 00:00:14.989
API itself and how it worked.

00:00:14.989 --> 00:00:19.880
So they have a not very
good documentation.

00:00:19.880 --> 00:00:25.459
So first thing was going through that
and understand how that service works.

00:00:25.509 --> 00:00:26.500
On stat was done.

00:00:26.599 --> 00:00:30.829
The implementation itself was a
little bit tricky because they didn't

00:00:30.829 --> 00:00:35.329
have filter in the API and they
didn't have pagination in the API.

00:00:35.359 --> 00:00:42.799
So you only had an option to
choose for how long you wanted

00:00:42.799 --> 00:00:44.966
to  fetch data in the past.

00:00:44.966 --> 00:00:48.656
So you can say, okay, I wanna
fetch 24 hours in the past.

00:00:49.421 --> 00:00:52.241
And the limit of the API
was always a thousand.

00:00:53.651 --> 00:00:59.634
So you could said, okay, I wanna have 48
hours, and the reply would be a thousand.

00:01:00.354 --> 00:01:03.474
But then you can have
more results in that API.

00:01:04.284 --> 00:01:10.434
So I implemented that calculation
in pagination itself, in our side.

00:01:10.434 --> 00:01:14.149
It would calculate if actually
the timestamp of the last result.

00:01:15.249 --> 00:01:18.579
Of that API would be actually 48 hours.

00:01:19.539 --> 00:01:25.279
Or do we need to query a little
bit more further with an offset

00:01:25.550 --> 00:01:27.770
of the first visit that we have.

00:01:27.770 --> 00:01:33.860
So we have always like the
exact time that we want to fetch

00:01:33.860 --> 00:01:36.919
the last 48 hours and then.

00:01:37.984 --> 00:01:42.634
After that, we have a bunch of data to,
to process in the Python application.

00:01:42.904 --> 00:01:50.021
And of course, once we have all the
data, we have to decide, okay, what

00:01:50.021 --> 00:01:54.241
are the actual new rows to transmit
to big query in this situation?

00:01:55.121 --> 00:01:58.961
Deduplication layer was also
built into the Python system to

00:01:58.961 --> 00:02:06.581
ensure that the bigQuery has only
unique data after the API fetch.

00:02:07.061 --> 00:02:12.801
So this was the stream and also
a separate table on BigQuery was

00:02:12.801 --> 00:02:16.101
created for the metadata of each load.

00:02:16.191 --> 00:02:19.461
So it saves some specific metadata.

00:02:20.136 --> 00:02:27.286
Not all data, not with before and
after that would be over and would

00:02:27.286 --> 00:02:29.656
maybe cost a little bit of money.

00:02:29.756 --> 00:02:32.336
However system was implemented.

00:02:32.336 --> 00:02:38.246
Now the report team can actually
just query data from BigQuery and get

00:02:38.246 --> 00:02:43.436
information about the their PBX and
call center information on PASCOM.

